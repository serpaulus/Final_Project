{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5291a42e",
   "metadata": {},
   "source": [
    "# Index of all potential Imports per Model\n",
    "#### Clean up later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae86535d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for Sergio > Machine Learing Model Test\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sqlalchemy import create_engine\n",
    "from config import db_password\n",
    "import psycopg2\n",
    "\n",
    "# Imports > Linear regression example\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Imports > Logistic Regression\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Imports > Logistic Regression Example - Predicting Diabetes\n",
    "from path import Path\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Imports > SVM Example - Loan Approver\n",
    "from path import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Imports > Example - Decision Trees\n",
    "import pandas as pd\n",
    "from path import Path\n",
    "from sklearn import tree\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "# Imports > Example - Random Forest Model\n",
    "import pandas as pd\n",
    "from path import Path\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "# Imports > Example - Gradient Boosted Tree\n",
    "import pandas as pd\n",
    "from path import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Imports > Example - Combination Sampling\n",
    "import pandas as pd\n",
    "from path import Path\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from imblearn.metrics import classification_report_imbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d137b0e",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7adbc44",
   "metadata": {},
   "source": [
    "# [Database options]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af616cbd",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa673d7",
   "metadata": {},
   "source": [
    "# Using AWS RDS, Spark, and Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2c5e1c",
   "metadata": {},
   "source": [
    "## Set up Relational Database and connect to pgAdmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ceef50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an AWS Relational Database 16.7.2\n",
    "# Connecting AWS to pgAdmin 16.7.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd5f051",
   "metadata": {},
   "source": [
    "## CRUD with AWS - Changing database stored in AWS cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c02e69d",
   "metadata": {},
   "source": [
    "## [In pgAdmin]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b97635",
   "metadata": {},
   "source": [
    "### Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807b3ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE TABLE doctors (\n",
    "#  id INT PRIMARY KEY NOT NULL,\n",
    "#  speciality TEXT,\n",
    "#  taking_patients BOOLEAN\n",
    "# );\n",
    "# CREATE TABLE patients (\n",
    "#  id INT NOT NULL,\n",
    "#  doctor_id INT NOT NULL,\n",
    "#  health_status TEXT,\n",
    "#  PRIMARY KEY (id, doctor_id),\n",
    "#  FOREIGN KEY (doctor_id) REFERENCES doctors (id)\n",
    "# );\n",
    "\n",
    "# INSERT INTO doctors(id, speciality, taking_patients)\n",
    "# VALUES\n",
    "# (1, 'cardiology', TRUE),\n",
    "# (2, 'orthopedics', FALSE),\n",
    "# (3, 'pediatrics', TRUE);\n",
    "# INSERT INTO patients (id, doctor_id, health_status)\n",
    "# VALUES\n",
    "# (1, 2, 'healthy'),\n",
    "# (2, 3, 'sick'),\n",
    "# (3, 2, 'sick'),\n",
    "# (4, 1, 'healthy'),\n",
    "# (5, 1, 'sick');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0888c3f",
   "metadata": {},
   "source": [
    "### Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f7561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Read tables\n",
    "SELECT * FROM doctors;\n",
    "SELECT * FROM patients;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb405f6",
   "metadata": {},
   "source": [
    "### Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd088c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Update rows\n",
    "UPDATE doctors\n",
    "SET taking_patients = FALSE\n",
    "WHERE id = 1;\n",
    "UPDATE patients\n",
    "SET health_status = 'healthy'\n",
    "WHERE id = 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5144e31a",
   "metadata": {},
   "source": [
    "### Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13dda16",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Delete row\n",
    "DELETE FROM patients\n",
    "WHERE id = 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1900de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load files into bucket\n",
    "# 16.9.1 PySpark ETL Full example saved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d115453",
   "metadata": {},
   "source": [
    " # PySpark ETL - using Google Colab and Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fe8033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data with AWS S3 16.8.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1986cc03",
   "metadata": {},
   "source": [
    "## [In pgAdmin]\n",
    "### Create tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c2d420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Create Active User Table in pdAdmin\n",
    "# CREATE TABLE active_user (\n",
    "#  id INT PRIMARY KEY NOT NULL,\n",
    "#  first_name TEXT,\n",
    "#  last_name TEXT,\n",
    "#  username TEXT\n",
    "# );\n",
    "\n",
    "# CREATE TABLE billing_info (\n",
    "#  billing_id INT PRIMARY KEY NOT NULL,\n",
    "#  street_address TEXT,\n",
    "#  state TEXT,\n",
    "#  username TEXT\n",
    "# );\n",
    "\n",
    "# CREATE TABLE payment_info (\n",
    "#  billing_id INT PRIMARY KEY NOT NULL,\n",
    "#  cc_encrypted TEXT\n",
    "# );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ac49ff",
   "metadata": {},
   "source": [
    "## [In Google Colab]\n",
    "### PySpark - Imports, Drivers, Required Installs, environment variables, create spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d51bded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Find the latest version of spark 3.0  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
    "# For example:\n",
    "# spark_version = 'spark-3.0.3'\n",
    "spark_version = 'spark-3.0.3'\n",
    "os.environ['SPARK_VERSION']=spark_version\n",
    "\n",
    "# Install Spark and Java\n",
    "!apt-get update\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark\n",
    "\n",
    "# Set Environment Variables\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
    "\n",
    "# Start a SparkSession\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662c34f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download postgres driver to allow Spark to interact with Posgres\n",
    "!wget https://jdbc.postgresql.org/download/postgresql-42.2.16.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c2b3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a Spark seesion with an additional option that adds the driver to Spark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"CloudETL\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.16.jar\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d223720",
   "metadata": {},
   "source": [
    "### PySpark ETL - EXTRACT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aa6aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data from S3 Buckets\n",
    "from pyspark import SparkFiles\n",
    "url =\"https://kwporras-bucket.s3.amazonaws.com/user_data.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "user_data_df = spark.read.csv(SparkFiles.get(\"user_data.csv\"), sep=\",\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899a919b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show DataFrame\n",
    "user_data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55055915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data from S3 buckets\n",
    "url =\"https://kwporras-bucket.s3.amazonaws.com/user_payment.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "user_payment_df = spark.read.csv(SparkFiles.get(\"user_payment.csv\"), sep=\",\", header=True, inferSchema=True)\n",
    "\n",
    "# Show DataFrame\n",
    "user_payment_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bab602",
   "metadata": {},
   "source": [
    "### PySpark ETL - TRANSFORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3178d27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the two DataFrame\n",
    "joined_df= user_data_df.join(user_payment_df, on=\"username\", how=\"inner\")\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f87c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop null values\n",
    "dropna_df = joined_df.dropna()\n",
    "dropna_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd103c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in a sql function to use columns\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter for only columns with active users\n",
    "cleaned_df = dropna_df.filter(col(\"active_user\") == True)\n",
    "cleaned_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7347687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create user dataframe to match active_user table\n",
    "clean_user_df = cleaned_df.select([\"id\", \"first_name\", \"last_name\", \"username\"])\n",
    "clean_user_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57756dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create user dataframe to match billing_info table\n",
    "clean_billing_df = cleaned_df.select([\"billing_id\", \"street_address\", \"State\", \"username\"])\n",
    "clean_billing_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3363373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create user dataframe to match payment_info table\n",
    "clean_payment_df = cleaned_df.select([\"billing_id\", \"cc_encrypted\"])\n",
    "clean_payment_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107baf6f",
   "metadata": {},
   "source": [
    "### PySpark ETL - LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe99e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store environmental variable\n",
    "from getpass import getpass\n",
    "password = getpass('Enter database password')\n",
    "# Configure settings for RDS\n",
    "mode = \"append\"\n",
    "jdbc_url=\"jdbc:postgresql://dataviz.czshayekq14i.us-east-2.rds.amazonaws.com:5432/my_data_class_db\"\n",
    "config = {\"user\":\"postgres\",\n",
    "          \"password\": password,\n",
    "          \"driver\":\"org.postgresql.Driver\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c0f45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write DataFrame to active_user table in RDS\n",
    "clean_user_df.write.jdbc(url=jdbc_url, table='active_user', mode=mode, properties=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839e1b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write dataframe to billing_info table in RDS\n",
    "clean_billing_df.write.jdbc(url=jdbc_url, table='billing_info', mode=mode, properties=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acbd804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write dataframe to payment_info table in RDS\n",
    "clean_payment_df.write.jdbc(url=jdbc_url, table='payment_info', mode=mode, properties=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765f318b",
   "metadata": {},
   "source": [
    "## [In pgAdmin]\n",
    "### -- Query database to check successful upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b68400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT * FROM active_user;\n",
    "# SELECT * FROM billing_info;\n",
    "# SELECT * FROM payment_info;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e9f753",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c9a5ba",
   "metadata": {},
   "source": [
    "# Alternative - Increase user accessiblilty to Database by avoiding postgreSQL\n",
    "### Use sqlalchemy and sqlite  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2326cb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "# Python SQL toolkit and Object Relational Mapper\n",
    "import sqlalchemy\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "from sqlalchemy.orm import Session\n",
    "from sqlalchemy import create_engine, func\n",
    "from sqlalchemy import extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c64afcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Up the Database engine for the Flask application to allow access to the SQLite database\n",
    "engine = create_engine(\"sqlite:///hawaii.sqlite\")\n",
    "\n",
    "# reflect an existing database into a new model\n",
    "Base = automap_base()\n",
    "# reflect the tables\n",
    "Base.prepare(engine, reflect=True)\n",
    "\n",
    "# Save references to each table\n",
    "Measurement = Base.classes.measurement\n",
    "Station = Base.classes.station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fc6272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our session (link) from Python to the DB\n",
    "session = Session(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f10448d",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b892251d",
   "metadata": {},
   "source": [
    "# Connecting Pandas and SQL 8.5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7447e96",
   "metadata": {},
   "source": [
    "### Create a Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ca9a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure a database in create in SQL covered above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e06c2b",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cbf259",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from config import db_password\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54999ec5",
   "metadata": {},
   "source": [
    "### Create the Database Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5313657c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"postgresql://[user]:[password]@[location]:[port]/[database]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1f4bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Practice - hide password in a config.py file and play config.py in .gitignore file\n",
    "db_password = 'YOUR_PASSWORD_HERE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079b0fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local server connection string\n",
    "db_string = f\"postgresql://postgres:{db_password}{db_password}@housing-prices.ctpruadwlamv.us-east-2.rds.amazonaws.com:5432/housing-prices\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e47254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the database engine\n",
    "engine = create_engine(db_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc49b7ba",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698ca292",
   "metadata": {},
   "source": [
    "# Machine Learning Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd872c96",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f495f35c",
   "metadata": {},
   "source": [
    "# Machine Learning Model Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16d99ae",
   "metadata": {},
   "source": [
    "## >>>[Linear Regression]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a6eb85",
   "metadata": {},
   "source": [
    "### 1. Split the data into input (X) and output (y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e0eadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format data to meet the requirements of the Scikit-learn library\n",
    "# Conventionally, the independent variable is placed on the x-axis, dependend on the y-axis\n",
    "# first argument of reshape() specifies the number of rows. -1 means number of rows is unspecified\n",
    "# second argument of reshape() refers to the number of columns. 1 means there is only one column of independnent variables\n",
    "# The data in the df column must be reshaped into an array with shape (num_samples, num_features)\n",
    "# https://stackoverflow.com/questions/18691084/what-does-1-mean-in-numpy-reshape\n",
    "X = df.YearsExperience.values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e3037f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine first five entries in X\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59923570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The shape of X is 30 samples, with a single feature (column)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fac4f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the Salary column as for the dependent variable. reshape() was not needed in this instance\n",
    "y = df.Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b89a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521d1969",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df30d1d",
   "metadata": {},
   "source": [
    "### 2. Create an instance of the model with model = LinearRegression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba0283e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# object instantiated, or created, from skylearn.linear_model's LinearGregresion class\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854f4d72",
   "metadata": {},
   "source": [
    "### 3. Train the model with the dataset with model.fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42c8488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning stage, alternatively called fitting or training\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ec7782",
   "metadata": {},
   "source": [
    "### 4. Create predictions with y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c07686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use predict() method to generate predictions\n",
    "y_pred = model.predict(X)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1124bb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predictions as a red line against the data\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X, y_pred, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41f7c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine specific parameters of model: the slope, model.ceof_, and the y-intercept, model.intercept_\n",
    "print(model.coef_)\n",
    "print(model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b41bef",
   "metadata": {},
   "source": [
    " # >>>[Logistic Regression]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aea16917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression is a statistical method for predicting binary outcomes from data.\n",
    "# Examples of this are \"yes\" vs \"no\" or \"high credit risk\" vs \"low credit risk\".\n",
    "# These are categories that translate to probability of being a 0 or a 1\n",
    "\n",
    "# We can calculate logistic regression by adding an activation function as the final step to our linear model.\n",
    "# This converts the linear regression output to a probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313ad88f",
   "metadata": {},
   "source": [
    "### 1. Create a model with LogisticRegression()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d346e728",
   "metadata": {},
   "source": [
    "#### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad66abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Path('Resources/diabetes.csv')\n",
    "df = pd.read_csv(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c33272",
   "metadata": {},
   "source": [
    " #### Separate the Features (X) from the Target (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9409a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"Outcome\"]\n",
    "X = df.drop(columns=\"Outcome\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2c803f",
   "metadata": {},
   "source": [
    " #### Split our data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb15ae69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    random_state=1, \n",
    "                                                    stratify=y)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecca21c6",
   "metadata": {},
   "source": [
    " #### Create a Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4782e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(solver='lbfgs',\n",
    "                                max_iter=200,\n",
    "                                random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e9b4c9",
   "metadata": {},
   "source": [
    "### 2. Train the model with model.fit()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1de7f5d",
   "metadata": {},
   "source": [
    " #### Fit (train) or model using the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94423fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da04d077",
   "metadata": {},
   "source": [
    "### 3. Make predictions with model.predict()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6108554",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "results = pd.DataFrame({\"Prediction\": y_pred, \"Actual\": y_test}).reset_index(drop=True)\n",
    "results.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76332fb",
   "metadata": {},
   "source": [
    "### 4. Validate the model with accuracy_score()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd40c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3687fa",
   "metadata": {},
   "source": [
    "#### Precision Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6638de5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4430687",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = confusion_matrix(y_test, y_pred)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284aab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b460fa",
   "metadata": {},
   "source": [
    "# >>>[SVM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c3fced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are a number of classification algorithms that can be used to determine loan elgibility.\n",
    "# Some algorithms run better than others. Build a loan approver using the SVM algorithm and compare the accuracy and\n",
    "# performance of the SVM model with the Logistic Regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddb3edf",
   "metadata": {},
   "source": [
    "### 1. Create a model with LogisticRegression()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6480296e",
   "metadata": {},
   "source": [
    "#### import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f13ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "# Note: The following data has been normalized between 0 and 1\n",
    "data = Path('Resources/loans_svm.csv')\n",
    "df = pd.read_csv(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c87ce4d",
   "metadata": {},
   "source": [
    " #### Separate the Features (X) from the Target (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62352ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment the features from the target\n",
    "y = df[\"status\"]\n",
    "X = df.drop(columns=\"status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0d0112",
   "metadata": {},
   "outputs": [],
   "source": [
    " #### Split our data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffcb1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the train_test_split function to create training and testing subsets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    random_state=1, \n",
    "                                                    stratify=y)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34e0891",
   "metadata": {},
   "source": [
    " #### Create a SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36325875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a linear SVM model\n",
    "from sklearn.svm import SVC\n",
    "model = SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6abc0c",
   "metadata": {},
   "source": [
    "### 2. Train the model with model.fit()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c2d419",
   "metadata": {},
   "source": [
    " #### Fit (train) or model using the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12840f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the data\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b84cf7",
   "metadata": {},
   "source": [
    "### 3. Make predictions with model.predict()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ad708d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the test data\n",
    "y_pred = model.predict(X_test)\n",
    "results = pd.DataFrame({\n",
    "    \"Prediction\": y_pred, \n",
    "    \"Actual\": y_test\n",
    "}).reset_index(drop=True)\n",
    "results.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb3790f",
   "metadata": {},
   "source": [
    "### 4. Validate the model with accuracy_score()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7960326c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1742f1",
   "metadata": {},
   "source": [
    "#### Generate Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e308ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b22efef",
   "metadata": {},
   "source": [
    "#### Generate Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5071263",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6be8308",
   "metadata": {},
   "source": [
    "# >>>[Decision Trees]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2c1cab",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805b957b",
   "metadata": {},
   "source": [
    "# Actual Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694f6930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data from PostgresSQL using SQLalchemy\n",
    "#Connect to PostgresSQL\n",
    "db_string = f\"postgresql://postgres:{db_password}@housing-prices.ctpruadwlamv.us-east-2.rds.amazonaws.com:5432/housing-prices\"\n",
    "engine = create_engine(db_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00213828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL Tables\n",
    "\n",
    "# clean_test\n",
    "# clean_test_df\n",
    "# clean_train\n",
    "# clean_train_df\n",
    "# features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767d4b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read\n",
    "df = pd.DataFrame(pd.read_sql_query(\"SELECT * FROM clean_test;\", engine, index_col='Id'))\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
