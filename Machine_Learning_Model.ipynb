{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5291a42e",
   "metadata": {},
   "source": [
    "# Index of all potential Imports per Model\n",
    "#### Clean up later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae86535d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for Sergio > Machine Learing Model Test\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sqlalchemy import create_engine\n",
    "from config import db_password\n",
    "import psycopg2\n",
    "\n",
    "# Imports > Linear regression example\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Imports > Logistic Regression\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Imports > Logistic Regression Example - Predicting Diabetes\n",
    "from path import Path\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Imports > SVM Example - Loan Approver\n",
    "from path import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Imports > Example - Decision Trees\n",
    "import pandas as pd\n",
    "from path import Path\n",
    "from sklearn import tree\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "# Imports > Example - Random Forest Model\n",
    "import pandas as pd\n",
    "from path import Path\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "# Imports > Example - Gradient Boosted Tree\n",
    "import pandas as pd\n",
    "from path import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Imports > Example - Combination Sampling\n",
    "import pandas as pd\n",
    "from path import Path\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from imblearn.metrics import classification_report_imbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d137b0e",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7adbc44",
   "metadata": {},
   "source": [
    "# [Database options]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af616cbd",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa673d7",
   "metadata": {},
   "source": [
    "# Using AWS RDS, Spark, and Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2c5e1c",
   "metadata": {},
   "source": [
    "## Set up Relational Database and connect to pgAdmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ceef50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an AWS Relational Database 16.7.2\n",
    "# Connecting AWS to pgAdmin 16.7.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd5f051",
   "metadata": {},
   "source": [
    "## CRUD with AWS - Changing database stored in AWS cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c02e69d",
   "metadata": {},
   "source": [
    "## [In pgAdmin]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b97635",
   "metadata": {},
   "source": [
    "### Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807b3ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE TABLE doctors (\n",
    "#  id INT PRIMARY KEY NOT NULL,\n",
    "#  speciality TEXT,\n",
    "#  taking_patients BOOLEAN\n",
    "# );\n",
    "# CREATE TABLE patients (\n",
    "#  id INT NOT NULL,\n",
    "#  doctor_id INT NOT NULL,\n",
    "#  health_status TEXT,\n",
    "#  PRIMARY KEY (id, doctor_id),\n",
    "#  FOREIGN KEY (doctor_id) REFERENCES doctors (id)\n",
    "# );\n",
    "\n",
    "# INSERT INTO doctors(id, speciality, taking_patients)\n",
    "# VALUES\n",
    "# (1, 'cardiology', TRUE),\n",
    "# (2, 'orthopedics', FALSE),\n",
    "# (3, 'pediatrics', TRUE);\n",
    "# INSERT INTO patients (id, doctor_id, health_status)\n",
    "# VALUES\n",
    "# (1, 2, 'healthy'),\n",
    "# (2, 3, 'sick'),\n",
    "# (3, 2, 'sick'),\n",
    "# (4, 1, 'healthy'),\n",
    "# (5, 1, 'sick');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0888c3f",
   "metadata": {},
   "source": [
    "### Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f7561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Read tables\n",
    "SELECT * FROM doctors;\n",
    "SELECT * FROM patients;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb405f6",
   "metadata": {},
   "source": [
    "### Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd088c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Update rows\n",
    "UPDATE doctors\n",
    "SET taking_patients = FALSE\n",
    "WHERE id = 1;\n",
    "UPDATE patients\n",
    "SET health_status = 'healthy'\n",
    "WHERE id = 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5144e31a",
   "metadata": {},
   "source": [
    "### Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13dda16",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Delete row\n",
    "DELETE FROM patients\n",
    "WHERE id = 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1900de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load files into bucket\n",
    "# 16.9.1 PySpark ETL Full example saved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d115453",
   "metadata": {},
   "source": [
    " # PySpark ETL - using Google Colab and Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fe8033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data with AWS S3 16.8.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1986cc03",
   "metadata": {},
   "source": [
    "## [In pgAdmin]\n",
    "### Create tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c2d420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Create Active User Table in pdAdmin\n",
    "# CREATE TABLE active_user (\n",
    "#  id INT PRIMARY KEY NOT NULL,\n",
    "#  first_name TEXT,\n",
    "#  last_name TEXT,\n",
    "#  username TEXT\n",
    "# );\n",
    "\n",
    "# CREATE TABLE billing_info (\n",
    "#  billing_id INT PRIMARY KEY NOT NULL,\n",
    "#  street_address TEXT,\n",
    "#  state TEXT,\n",
    "#  username TEXT\n",
    "# );\n",
    "\n",
    "# CREATE TABLE payment_info (\n",
    "#  billing_id INT PRIMARY KEY NOT NULL,\n",
    "#  cc_encrypted TEXT\n",
    "# );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ac49ff",
   "metadata": {},
   "source": [
    "## [In Google Colab]\n",
    "### PySpark - Imports, Drivers, Required Installs, environment variables, create spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d51bded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Find the latest version of spark 3.0  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
    "# For example:\n",
    "# spark_version = 'spark-3.0.3'\n",
    "spark_version = 'spark-3.0.3'\n",
    "os.environ['SPARK_VERSION']=spark_version\n",
    "\n",
    "# Install Spark and Java\n",
    "!apt-get update\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark\n",
    "\n",
    "# Set Environment Variables\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
    "\n",
    "# Start a SparkSession\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662c34f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download postgres driver to allow Spark to interact with Posgres\n",
    "!wget https://jdbc.postgresql.org/download/postgresql-42.2.16.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c2b3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a Spark seesion with an additional option that adds the driver to Spark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"CloudETL\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.16.jar\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d223720",
   "metadata": {},
   "source": [
    "### PySpark ETL - EXTRACT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aa6aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data from S3 Buckets\n",
    "from pyspark import SparkFiles\n",
    "url =\"https://kwporras-bucket.s3.amazonaws.com/user_data.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "user_data_df = spark.read.csv(SparkFiles.get(\"user_data.csv\"), sep=\",\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899a919b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show DataFrame\n",
    "user_data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55055915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data from S3 buckets\n",
    "url =\"https://kwporras-bucket.s3.amazonaws.com/user_payment.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "user_payment_df = spark.read.csv(SparkFiles.get(\"user_payment.csv\"), sep=\",\", header=True, inferSchema=True)\n",
    "\n",
    "# Show DataFrame\n",
    "user_payment_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bab602",
   "metadata": {},
   "source": [
    "### PySpark ETL - TRANSFORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3178d27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the two DataFrame\n",
    "joined_df= user_data_df.join(user_payment_df, on=\"username\", how=\"inner\")\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f87c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop null values\n",
    "dropna_df = joined_df.dropna()\n",
    "dropna_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd103c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in a sql function to use columns\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter for only columns with active users\n",
    "cleaned_df = dropna_df.filter(col(\"active_user\") == True)\n",
    "cleaned_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7347687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create user dataframe to match active_user table\n",
    "clean_user_df = cleaned_df.select([\"id\", \"first_name\", \"last_name\", \"username\"])\n",
    "clean_user_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57756dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create user dataframe to match billing_info table\n",
    "clean_billing_df = cleaned_df.select([\"billing_id\", \"street_address\", \"State\", \"username\"])\n",
    "clean_billing_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3363373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create user dataframe to match payment_info table\n",
    "clean_payment_df = cleaned_df.select([\"billing_id\", \"cc_encrypted\"])\n",
    "clean_payment_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107baf6f",
   "metadata": {},
   "source": [
    "### PySpark ETL - LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe99e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store environmental variable\n",
    "from getpass import getpass\n",
    "password = getpass('Enter database password')\n",
    "# Configure settings for RDS\n",
    "mode = \"append\"\n",
    "jdbc_url=\"jdbc:postgresql://dataviz.czshayekq14i.us-east-2.rds.amazonaws.com:5432/my_data_class_db\"\n",
    "config = {\"user\":\"postgres\",\n",
    "          \"password\": password,\n",
    "          \"driver\":\"org.postgresql.Driver\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c0f45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write DataFrame to active_user table in RDS\n",
    "clean_user_df.write.jdbc(url=jdbc_url, table='active_user', mode=mode, properties=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839e1b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write dataframe to billing_info table in RDS\n",
    "clean_billing_df.write.jdbc(url=jdbc_url, table='billing_info', mode=mode, properties=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acbd804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write dataframe to payment_info table in RDS\n",
    "clean_payment_df.write.jdbc(url=jdbc_url, table='payment_info', mode=mode, properties=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765f318b",
   "metadata": {},
   "source": [
    "## [In pgAdmin]\n",
    "### -- Query database to check successful upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b68400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT * FROM active_user;\n",
    "# SELECT * FROM billing_info;\n",
    "# SELECT * FROM payment_info;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e9f753",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c9a5ba",
   "metadata": {},
   "source": [
    "# Alternative - Increase user accessiblilty to Database by avoiding postgreSQL\n",
    "### Use sqlalchemy and sqlite  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2326cb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "# Python SQL toolkit and Object Relational Mapper\n",
    "import sqlalchemy\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "from sqlalchemy.orm import Session\n",
    "from sqlalchemy import create_engine, func\n",
    "from sqlalchemy import extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c64afcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Up the Database engine for the Flask application to allow access to the SQLite database\n",
    "engine = create_engine(\"sqlite:///hawaii.sqlite\")\n",
    "\n",
    "# reflect an existing database into a new model\n",
    "Base = automap_base()\n",
    "# reflect the tables\n",
    "Base.prepare(engine, reflect=True)\n",
    "\n",
    "# Save references to each table\n",
    "Measurement = Base.classes.measurement\n",
    "Station = Base.classes.station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fc6272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our session (link) from Python to the DB\n",
    "session = Session(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f10448d",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b892251d",
   "metadata": {},
   "source": [
    "# Connecting Pandas and SQL 8.5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7447e96",
   "metadata": {},
   "source": [
    "### Create a Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ca9a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure a database in create in SQL covered above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e06c2b",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cbf259",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from config import db_password\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54999ec5",
   "metadata": {},
   "source": [
    "### Create the Database Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5313657c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"postgresql://[user]:[password]@[location]:[port]/[database]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1f4bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Practice - hide password in a config.py file and play config.py in .gitignore file\n",
    "db_password = 'YOUR_PASSWORD_HERE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079b0fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local server connection string\n",
    "db_string = f\"postgresql://postgres:{db_password}{db_password}@housing-prices.ctpruadwlamv.us-east-2.rds.amazonaws.com:5432/housing-prices\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e47254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the database engine\n",
    "engine = create_engine(db_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc49b7ba",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698ca292",
   "metadata": {},
   "source": [
    "# Machine Learning Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd872c96",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f495f35c",
   "metadata": {},
   "source": [
    "# Machine Learning Model Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16d99ae",
   "metadata": {},
   "source": [
    "## >>>[Linear Regression]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de84d79",
   "metadata": {},
   "source": [
    "### 1. Create a model - Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c0dd28",
   "metadata": {},
   "source": [
    "#### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f390049a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV file as Pandas DataFrame\n",
    "df = pd.read_csv(Path('Resources/Salary_Data.csv'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a408611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect relationship between Year of Experience and Salary\n",
    "plt.scatter(df.YearsExperience, df.Salary)\n",
    "plt.xlabel('Years of Experience')\n",
    "plt.ylabel('Salary in USD')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a6eb85",
   "metadata": {},
   "source": [
    "#### Split the data into input (X) and output (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e0eadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format data to meet the requirements of the Scikit-learn library\n",
    "# Conventionally, the independent variable is placed on the x-axis, dependend on the y-axis\n",
    "# first argument of reshape() specifies the number of rows. -1 means number of rows is unspecified\n",
    "# second argument of reshape() refers to the number of columns. 1 means there is only one column of independnent variables\n",
    "# The data in the df column must be reshaped into an array with shape (num_samples, num_features)\n",
    "# https://stackoverflow.com/questions/18691084/what-does-1-mean-in-numpy-reshape\n",
    "X = df.YearsExperience.values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e3037f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine first five entries in X\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59923570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The shape of X is 30 samples, with a single feature (column)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fac4f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the Salary column as for the dependent variable. reshape() was not needed in this instance\n",
    "y = df.Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b89a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521d1969",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df30d1d",
   "metadata": {},
   "source": [
    "#### Create a Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba0283e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# object instantiated, or created, from skylearn.linear_model's LinearGregresion class\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854f4d72",
   "metadata": {},
   "source": [
    "### 2. Train the model with model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42c8488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning stage, alternatively called fitting or training\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ec7782",
   "metadata": {},
   "source": [
    "### 3. Make predictions with model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c07686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use predict() method to generate predictions\n",
    "y_pred = model.predict(X)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1124bb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predictions as a red line against the data\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X, y_pred, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41f7c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine specific parameters of model: the slope, model.ceof_, and the y-intercept, model.intercept_\n",
    "print(model.coef_)\n",
    "print(model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b41bef",
   "metadata": {},
   "source": [
    " # >>>[Logistic Regression]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aea16917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression is a statistical method for predicting binary outcomes from data.\n",
    "# Examples of this are \"yes\" vs \"no\" or \"high credit risk\" vs \"low credit risk\".\n",
    "# These are categories that translate to probability of being a 0 or a 1\n",
    "\n",
    "# We can calculate logistic regression by adding an activation function as the final step to our linear model.\n",
    "# This converts the linear regression output to a probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313ad88f",
   "metadata": {},
   "source": [
    "### 1. Create a model - Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d346e728",
   "metadata": {},
   "source": [
    "#### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad66abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Path('Resources/diabetes.csv')\n",
    "df = pd.read_csv(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c33272",
   "metadata": {},
   "source": [
    " #### Separate the Features (X) from the Target (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9409a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"Outcome\"]\n",
    "X = df.drop(columns=\"Outcome\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2c803f",
   "metadata": {},
   "source": [
    " #### Split our data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb15ae69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    random_state=1, \n",
    "                                                    stratify=y)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecca21c6",
   "metadata": {},
   "source": [
    " #### Create a Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4782e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(solver='lbfgs',\n",
    "                                max_iter=200,\n",
    "                                random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e9b4c9",
   "metadata": {},
   "source": [
    "### 2. Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1de7f5d",
   "metadata": {},
   "source": [
    " #### Fit (train) or model using the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94423fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da04d077",
   "metadata": {},
   "source": [
    "### 3. Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6108554",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "results = pd.DataFrame({\"Prediction\": y_pred, \"Actual\": y_test}).reset_index(drop=True)\n",
    "results.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76332fb",
   "metadata": {},
   "source": [
    "### 4. Validate the model with Accuracy Score, Confusion Matrix, and Generate Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd40c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3687fa",
   "metadata": {},
   "source": [
    "#### Precision Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6638de5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4430687",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = confusion_matrix(y_test, y_pred)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284aab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b460fa",
   "metadata": {},
   "source": [
    "# >>>[Support vector machine (SVM)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c3fced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are a number of classification algorithms that can be used to determine loan elgibility.\n",
    "# Some algorithms run better than others. Build a loan approver using the SVM algorithm and compare the accuracy and\n",
    "# performance of the SVM model with the Logistic Regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddb3edf",
   "metadata": {},
   "source": [
    "### 1. Create a model - Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6480296e",
   "metadata": {},
   "source": [
    "#### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f13ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "# Note: The following data has been normalized between 0 and 1\n",
    "data = Path('Resources/loans_svm.csv')\n",
    "df = pd.read_csv(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c87ce4d",
   "metadata": {},
   "source": [
    " #### Separate the Features (X) from the Target (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62352ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment the features from the target\n",
    "y = df[\"status\"]\n",
    "X = df.drop(columns=\"status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66e2846",
   "metadata": {},
   "source": [
    " #### Split our data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffcb1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the train_test_split function to create training and testing subsets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    random_state=1, \n",
    "                                                    stratify=y)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34e0891",
   "metadata": {},
   "source": [
    " #### Create a SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36325875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a linear SVM model\n",
    "from sklearn.svm import SVC\n",
    "model = SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6abc0c",
   "metadata": {},
   "source": [
    "### 2. Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c2d419",
   "metadata": {},
   "source": [
    " #### Fit (train) or model using the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12840f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the data\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b84cf7",
   "metadata": {},
   "source": [
    "### 3. Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ad708d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the test data\n",
    "y_pred = model.predict(X_test)\n",
    "results = pd.DataFrame({\n",
    "    \"Prediction\": y_pred, \n",
    "    \"Actual\": y_test\n",
    "}).reset_index(drop=True)\n",
    "results.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb3790f",
   "metadata": {},
   "source": [
    "### 4. Validate the model with Accuracy Score, Confusion Matrix, and Classification Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aebd434",
   "metadata": {},
   "source": [
    "#### Generate accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72c5ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1742f1",
   "metadata": {},
   "source": [
    "#### Generate Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e308ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b22efef",
   "metadata": {},
   "source": [
    "#### Generate Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5071263",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6be8308",
   "metadata": {},
   "source": [
    "# >>>[Decision Trees]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5679aa",
   "metadata": {},
   "source": [
    "### 1. Create a model - Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79673a25",
   "metadata": {},
   "source": [
    "#### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042e7543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "file_path = Path(\"Resources/loans_data_encoded_decision_trees.csv\")\n",
    "df_loans = pd.read_csv(file_path)\n",
    "df_loans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f2bf1d",
   "metadata": {},
   "source": [
    " #### Separate the Features (X) from the Target (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8997ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features set\n",
    "X = df_loans.copy()\n",
    "X = X.drop(\"bad\", axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37167759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target vector\n",
    "y = df_loans[\"bad\"].values.reshape(-1, 1)\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef080d36",
   "metadata": {},
   "source": [
    " #### Split our data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95e20a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into Train and Test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f635e7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c389ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, random_state=78, train_size=0.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5297846d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train2.shape)\n",
    "print(X_test2.shape)\n",
    "print(y_train2.shape)\n",
    "print(y_test2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1057a6b8",
   "metadata": {},
   "source": [
    " #### Create a Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384f6922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating StandardScaler instance\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c739d2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Standard Scaller\n",
    "X_scaler = scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5327b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da619a6b",
   "metadata": {},
   "source": [
    "### 2. Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068a9132",
   "metadata": {},
   "source": [
    " #### Fit (train) or model using the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9b00c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the decision tree classifier instance\n",
    "model = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4cbb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model\n",
    "model = model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73e2619",
   "metadata": {},
   "source": [
    "### 3. Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5970b725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions using the testing data\n",
    "predictions = model.predict(X_test_scaled)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b2bbee",
   "metadata": {},
   "source": [
    "### 4. Validate the model with Accuracy Score, Confusion Matrix, and Classification Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf4b133",
   "metadata": {},
   "source": [
    "#### Generate Accuracy Score & Confusion Matrix & Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbbfcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "cm_df = pd.DataFrame(\n",
    "    cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"]\n",
    ")\n",
    "print(cm_df)\n",
    "# Calculating the accuracy score\n",
    "acc_score = accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b106e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying results\n",
    "print(\"Confusion Matrix\")\n",
    "display(cm_df)\n",
    "print(f\"Accuracy Score : {acc_score}\")\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e037699f",
   "metadata": {},
   "source": [
    "# >>>[Random Forest Model]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1235a29",
   "metadata": {},
   "source": [
    "### 1. Create a model - Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92585bc4",
   "metadata": {},
   "source": [
    "#### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e79e256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "file_path = Path(\"Resources/loans_data_encoded_random_forest_model.csv\")\n",
    "df_loans = pd.read_csv(file_path)\n",
    "df_loans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d5b681",
   "metadata": {},
   "source": [
    " #### Separate the Features (X) from the Target (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6332a218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features set\n",
    "X = df_loans.copy()\n",
    "X = X.drop(\"bad\", axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aed570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target set.\n",
    "y = df_loans[\"bad\"].ravel()\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e769a6",
   "metadata": {},
   "source": [
    " #### Split our data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9fc0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into Train and Test sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfab049",
   "metadata": {},
   "source": [
    " #### Create a Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c795edf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a StandardScaler instance.\n",
    "scaler = StandardScaler()\n",
    "# Fitting the Standard Scaler with the training data.\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scaling the data.\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cffc1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random forest classifier.\n",
    "rf_model = RandomForestClassifier(n_estimators=500, random_state=78) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c3579e",
   "metadata": {},
   "source": [
    "### 2. Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1150d33a",
   "metadata": {},
   "source": [
    " #### Fit (train) or model using the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e611e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model\n",
    "rf_model = rf_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7a65bb",
   "metadata": {},
   "source": [
    "### 3. Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47617940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions using the testing data.\n",
    "predictions = rf_model.predict(X_test_scaled)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815217c8",
   "metadata": {},
   "source": [
    "### 4. Validate the model with Accuracy Score, Confusion Matrix, Classification Report and Rank Importance of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03bb29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the confusion matrix.\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "# Create a DataFrame from the confusion matrix.\n",
    "cm_df = pd.DataFrame(\n",
    "    cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n",
    "\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa64e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the accuracy score.\n",
    "acc_score = accuracy_score(y_test, predictions)\n",
    "acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c2909f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying results\n",
    "print(\"Confusion Matrix\")\n",
    "display(cm_df)\n",
    "print(f\"Accuracy Score : {acc_score}\")\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8510af",
   "metadata": {},
   "source": [
    "#### Rank the Importnace of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aa1367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature importance in the Random Forest model.\n",
    "importances = rf_model.feature_importances_\n",
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4f5166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can sort the features by their importance.\n",
    "sorted(zip(rf_model.feature_importances_, X.columns), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67695cac",
   "metadata": {},
   "source": [
    "# >>>[Gradient Boosted Tree]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f8fd87",
   "metadata": {},
   "source": [
    "### 1. Create a model - Gradient Boosted Tree Model (Preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e4278a",
   "metadata": {},
   "source": [
    "#### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac88056a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "file_path = Path(\"Resources/loans_data_encoded_gradient_boosted_tree.csv\")\n",
    "loans_df = pd.read_csv(file_path)\n",
    "loans_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4980b5",
   "metadata": {},
   "source": [
    "#### Separate the Features (X) from the Target (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e6e597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features set\n",
    "X = loans_df.copy()\n",
    "X = X.drop(\"bad\", axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09413d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target vector\n",
    "y = loans_df[\"bad\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa63042",
   "metadata": {},
   "source": [
    "#### Split our data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096a5ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Splitting into Train and Test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffbf013",
   "metadata": {},
   "source": [
    "#### Create a Gradient Boosted Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82611326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fitting Standard Scaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scaling data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f156eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Create a classifier object\n",
    "learning_rates = [0.05, 0.1, 0.25, 0.5, 0.75, 1]\n",
    "for learning_rate in learning_rates:\n",
    "    classifier = GradientBoostingClassifier(n_estimators=20,\n",
    "                                            learning_rate=learning_rate,\n",
    "                                            max_features=5,\n",
    "                                            max_depth=3,\n",
    "                                            random_state=0)\n",
    "\n",
    "    # Fit the model\n",
    "    classifier.fit(X_train_scaled, y_train)\n",
    "    print(\"Learning rate: \", learning_rate)\n",
    "\n",
    "    # Score the model\n",
    "    print(\"Accuracy score (training): {0:.3f}\".format(\n",
    "        classifier.score(\n",
    "            X_train_scaled,\n",
    "            y_train)))\n",
    "    print(\"Accuracy score (validation): {0:.3f}\".format(\n",
    "        classifier.score(\n",
    "            X_test_scaled,\n",
    "            y_test)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f2ba82",
   "metadata": {},
   "source": [
    "### 2. Train the model (Choosing Best Learning Rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508b3595",
   "metadata": {},
   "source": [
    "#### Fit (train) or model using the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60faf61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Create a classifier object\n",
    "learning_rates = [0.05, 0.1, 0.25, 0.5, 0.75, 1]\n",
    "for learning_rate in learning_rates:\n",
    "    classifier = GradientBoostingClassifier(n_estimators=20,\n",
    "                                            learning_rate=learning_rate,\n",
    "                                            max_features=5,\n",
    "                                            max_depth=3,\n",
    "                                            random_state=0)\n",
    "\n",
    "    # Fit the model\n",
    "    classifier.fit(X_train_scaled, y_train)\n",
    "    print(\"Learning rate: \", learning_rate)\n",
    "\n",
    "    # Score the model\n",
    "    print(\"Accuracy score (training): {0:.3f}\".format(\n",
    "        classifier.score(\n",
    "            X_train_scaled,\n",
    "            y_train)))\n",
    "    print(\"Accuracy score (validation): {0:.3f}\".format(\n",
    "        classifier.score(\n",
    "            X_test_scaled,\n",
    "            y_test)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14f8d00",
   "metadata": {},
   "source": [
    "### 3. Make predictions (Create Gradient Boosting Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b1aab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a learning rate and create classifier\n",
    "classifier = GradientBoostingClassifier(n_estimators=20,\n",
    "                                        learning_rate=0.5,\n",
    "                                        max_features=5,\n",
    "                                        max_depth=3,\n",
    "                                        random_state=0)\n",
    "\n",
    "# Fit the model\n",
    "classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make Prediction\n",
    "predictions = classifier.predict(X_test_scaled)\n",
    "pd.DataFrame({\"Prediction\": predictions, \"Actual\": y_test}).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9670bd39",
   "metadata": {},
   "source": [
    "### 4. Validate the model with Accuracy Score, Confusion Matrix, Classification Report and Rank Importance of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b850570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "# Calculating the accuracy score\n",
    "acc_score = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy Score : {acc_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458d39b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "cm_df = pd.DataFrame(\n",
    "    cm, index=[\"Actual 0\", \"Actual 1\"],\n",
    "    columns=[\"Predicted 0\", \"Predicted 1\"]\n",
    ")\n",
    "\n",
    "# Displaying results\n",
    "display(cm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e5e7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification report\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d960dde3",
   "metadata": {},
   "source": [
    "# >>>[Combination Sampling Model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68f41af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the SMOTEENN technique with the credit card default data.\n",
    "# Then estimate a logistic regression model and report the classification evaluation metrics.\n",
    "# ln_balance_limit is the log of the maximum balance they can have on the card;\n",
    "# 1 is female, 0 male for sex; the education is denoted: 1 = graduate school; 2 = university; 3 = high school;\n",
    "# 4 = others; 1 is married and 0 single for marriage; default_next_month is whether the person defaults in the following month (1 yes, 0 no)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f376aa",
   "metadata": {},
   "source": [
    "### 1. Create a model - Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40d9698",
   "metadata": {},
   "source": [
    "#### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4aa58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Path('Resources/cc_default_combination_sampling.csv')\n",
    "df = pd.read_csv(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cc36bc",
   "metadata": {},
   "source": [
    "#### Separate the Features (X) from the Target (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45147b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = [i for i in df.columns if i not in ('ID', 'default_next_month')]\n",
    "X = df[x_cols]\n",
    "y = df['default_next_month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e9ce1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d37b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1745d48",
   "metadata": {},
   "source": [
    "#### Split our data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071d3b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff78bba",
   "metadata": {},
   "source": [
    "#### Create a Combination sampling with SMOTEENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5315515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the SMOTEENN technique to perform combination sampling on the data\n",
    "# Count the resampled classes\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "smote_enn = SMOTEENN(random_state=0)\n",
    "X_resampled, y_resampled = smote_enn.fit_resample(X, y)\n",
    "Counter(y_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68b7eb8",
   "metadata": {},
   "source": [
    "### 2. Train the model (Logistic Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008f1f5f",
   "metadata": {},
   "source": [
    "#### Fit (train) or model using the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758a3185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a Logistic regression model using random undersampled data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(solver='lbfgs', random_state=1)\n",
    "model.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d655392e",
   "metadata": {},
   "source": [
    "### 3. Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f1c115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c08d14a",
   "metadata": {},
   "source": [
    "### 4. Validate the model with Accuracy Score, Confusion Matrix, Classification Report and Rank Importance of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6695dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Balanced Accuracy Score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "balanced_accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45abc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the imbalanced classification report\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "print(classification_report_imbalanced(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737da69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. Create a model - Random Forest Model\n",
    "#### Import data\n",
    "#### Separate the Features (X) from the Target (y)\n",
    "#### Split our data into training and testing\n",
    "#### Create a Random Forest Model\n",
    "### 2. Train the model\n",
    "#### Fit (train) or model using the training data\n",
    "### 3. Make predictions\n",
    "### 4. Validate the model with Accuracy Score, Confusion Matrix, Classification Report and Rank Importance of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ca8597",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e2c1cab",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805b957b",
   "metadata": {},
   "source": [
    "# Actual Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694f6930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data from PostgresSQL using SQLalchemy\n",
    "#Connect to PostgresSQL\n",
    "db_string = f\"postgresql://postgres:{db_password}@housing-prices.ctpruadwlamv.us-east-2.rds.amazonaws.com:5432/housing-prices\"\n",
    "engine = create_engine(db_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00213828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL Tables\n",
    "\n",
    "# clean_test\n",
    "# clean_test_df\n",
    "# clean_train\n",
    "# clean_train_df\n",
    "# features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767d4b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read\n",
    "df = pd.DataFrame(pd.read_sql_query(\"SELECT * FROM clean_test;\", engine, index_col='Id'))\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
